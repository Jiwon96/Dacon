# [private 1위](https://dacon.io/competitions/official/236035/codeshare/7430?page=1&dtype=recent)
* Encoding
  * ordinal Encoding:
    * 설문조사에서 1,2,3,4 등의 점수는 order를 가진 카테고리 변수들임
  * binary encording:
    * 원 핫 인코딩보다 적은 저장공간으로 원핫 인코딩처럼 표현 가능
* SMOTE:
  * 등장 아이디어: 모델을 학습시킴에 있어서 클래스간 샘플 수의 차이가 너무 크면 분류기는 많은 샘플이 존재하는 클래스로 편향됨 -> 재현율(recall)이 작아짐
  * 데이터의 불균형을 해결해주는 라이브러리(오버샘플링 기법임)
    * Synthetic Minority Oversampling Technique(SMOTE)
        * 소수 범주의 데이터를 무작위로 선택해 knn으로 군집을 만듬, 군집 내 한 점을 X(NN)이라 둘 때<br> $X_{Synthetic} = X + M{(X_{NN-X})}$ 로 새로운 데이터를 만듬![image](https://github.com/Jiwon96/Dacon/assets/65645796/e2ebe61c-d531-4746-8911-66e0a3b9eff1)
     
    * Category_encorders: 범주형 변수를 숫자로 변환해주는 라이브러리
      
    * Borderline SMOTE: 범주 간의 경계 부분에만 오버 샘플링을 하는 것<br>![image](https://github.com/Jiwon96/Dacon/assets/65645796/d6a0d337-98cd-4b5d-bfa5-6585f90dc4b0)
    * Voting Classifier: classifer의 결과들을 집계하여 가장 많은 표를 얻는 클래스를 최종 예측값으로 정함.
```python
    models = [
    ('bag', BaggingClassifier(random_state=CFG.SEED)),
    ('dt', DecisionTreeClassifier(random_state=CFG.SEED)),
    ('rc', RidgeClassifier(random_state=CFG.SEED)),
    ('xgb', XGBClassifier(random_state=CFG.SEED)),
    ('lgb', LGBMClassifier(random_state=CFG.SEED)),
    ('gb', GradientBoostingClassifier(random_state=CFG.SEED)),
    ('svc', SVC(random_state=CFG.SEED)),
    ('rcc', RidgeClassifierCV()),
    ('rf', RandomForestClassifier(random_state=CFG.SEED))
]
#최종모델은 Votingclassifier 사용하여 ensemble -> 제출결과 public score기준 XGBClassifier와 RandomForestClassifier 성능이 좋아 가중치를 주었음
best_model  = VotingClassifier(models, voting='hard', weights=[1,1,1,2,1,1,1,1,2])
best_model.fit(train_x,train_y)
```

      
# [private 2위](https://dacon.io/competitions/official/236035/codeshare/7487?page=2&dtype=recent)
# [private 3위](https://dacon.io/competitions/official/236035/codeshare/7435?page=1&dtype=recent)
# [private 4위](https://dacon.io/competitions/official/236035/codeshare/7429?page=2&dtype=recent)
# [private 5위](https://dacon.io/competitions/official/236035/codeshare/7488?page=2&dtype=recent)
# [번외 auto ml](https://dacon.io/competitions/official/236035/codeshare/7413?page=1&dtype=recent)
* <b> Auto ML</b>
  * pycaret:
    * setup: data에 전처리를 해줌 but 데이터를 추가하거나 등등 그런 기능은 없으므로 아마 private 1위 코드에서 데이터를 붙인다음에 하면 더 성능이 향상될 듯
    * models(): 사용 가능한 모델 확인 이 떄 사용가능한 모델을 가져와서 파라미터 조정도 가능함
    * compare_models(): models()에서 사용가능한 모델의 성능 비교
    * tune_model(): compare_models에서 n_select를 활용하면 해당 모델만큼 선택이 되는데, 그 모델을 다시 튜닝해주는 함수임, n_iter 값이 커질수록 성능이 좋아지지만, 시간 오래, optimizer는 어떤 metric으로 scoring 할 지 알려줌
    * blend_model(): 앙상블 모델을 줌 etimatior_list에 사용하고자 하는 모델 리스트를 넣어줌, fold, method: optimizer의 파라미터가 있음.
    * finalize_model() 모델 성능평가 하는 것, evaluate_model() 함수 사용하면 feature_importance 등을 확인할 수 있음
    * predict_model(): 예측하는 것
  * H2O AutoML
    * h2o.init(): h2o 인스턴스 생성함
    * h2o.H20Frame(): h2o에는 자체 프레임이 있어서 이를 통해서 예측하거나 등등 할 수 있음. 자체적으로 preprocessing을 해줌
    * H20AutoML: 모델을 만들어주는 파라미터로 max_runtime_secs 탐색 시간을 정의하는 파라미터와, exclude_algo 알고리즘을 제외할 파라미터를 선언할 수 있음
    * import_file: H2Oautoml을 통해서 만든 model을 import해서 가져옴
    * predict
  * LightAutoML
   * 이건 colab에서 계속 에러뜸
  * TPOT
    * 일반적으로 오랜 시간이 들기 때문에 가장 좋은 성능을 낸다고 알려짐
    * 이 library는 다른 라이브러리와는 다르게 데이터 전처리 과정을 거쳐야됨
    * custom_score를 활용할 수 있고, TPOTClassifier을 활용해서 모델을 fitting 할 수 있음
      * TPOTClassifier에는 다음과 같은 파라미터 있음 generations: 몇 세대를 걸쳐서 모델을 탐색? 클수록 높은 성능을 찾지만, 오래 걸림
      * population_size: 매 generation마다 남겨놓을 pipeline 수?
      * offspring_size: 매 generation마다 생성한 자손의 수
      * 즉 generation*offspring_size + population_size로 pipeline 평가
      * scoring, cv, subsample, max_time_mins, max_eval_time_mins 등등 파라미터가 있음
      * 
